{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dimensionality Reduction & Feature Selection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecFWfS3vMckw",
        "colab_type": "text"
      },
      "source": [
        "<b><u><title>Redukcja wymiarów oraz selekcja cech kluczowych wielowymiarowych zbiorów dancyh.</title></b></u>\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "# Krótkie słowa wstępu.\n",
        "\n",
        "Uprawianie Data Science w bioinformatyce oraz innych wymagających cierpliwości, specjalistycznej wiedzy oraz dotczących relanych problemów świata dziedzin nauki jest wciąż raczkującym podejściem. Rozumienie stosowanych metod niekiedy schodzi na dalszy plan co jest skutkiem goniących terminów oraz rzekomej złożoności matematyki stojącej za owymi rozwiązaniami.\n",
        "Niekiedy metody te oparte na prostej statystyce, będąc zachwalane w publikacjach, tutorialach oraz na przestrzeniu forum internetowych nie powinny mieć zastosowania do większości danych biologicznych, lecz stanowią swoisty \"trigger word\", który ma za zadanie pokazać że badacz jest na czasie, \"cool\" oraz nie boi się interdyscyplinarności.\n",
        "\n",
        "\n",
        "## Principal Component / Coordinate Analysis, czyli jak być fajnym ale nie koniecznie poprawnym.\n",
        "\n",
        "Nie raz rozmawiając z osobami potencjalnie zajmującymi się podobnymi problemami badawczymi (biostatystycy, data scientists, biolodzy) natykałem się ze słowami rodem z pasty o [fanatyku wędkarstwa](https://copypasta.fandom.com/pl/wiki/M%C3%B3j_stary_to_fanatyk_w%C4%99dkarstwa.) \"PCA jest król genomiki, tak jak lew jest król dżungli!\". Jeśli słowa te pozostawiają u was cierpki wyraz twarzy, nie spojlerujcie mniej zaznajomionym kolegom dlaczego. Dzisiaj sami dojdą do tych wniosków.\n",
        "\n",
        "## Podstawa użytkowania PCA\n",
        "\n",
        "Jeśli wkleiłbym tutaj wzór na PCA, połowa z was by zasnęła, 1/4 się zaśmiała a 1/4 wyszła. Wklejanie tak skomplikowanych wzorów mija się z celem. Jeśli ktoś z was będzie chciał faktycznie zgłębić jego tajemnicę to [tutaj](https://www.youtube.com/watch?v=kw9R0nD69OU) oraz [tam](https://www.youtube.com/watch?v=rng04VJxUt4) znajdziecie ważniejsze informacje na temat jego działania. Moim dzisiejszym zadaniem jest przekazanie wam intuicji dotyczącej tego kiedy używać tego algorytmu a kiedy nie. Zatem do rzeczy, macie 5 minut żeby przypomnieć sobie kluczową różnicę pomiędzy testem parametrycznym takim jak ANOVA a nie parametrycznym takim jak Kruskal-Wallis test. Uścisk ręki prezesa temu kto najprościej powie jak sprawdzić kiedy stosować jeden test a kiedy drugi. Czas start! Odpalamy komórkę poniżej i lecimy z tematem!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvEIrnUuRJ3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "for i in range(5):\n",
        "  if i < 4:\n",
        "    print(\"{} min left\".format(5 - i))\n",
        "    time.sleep(60)\n",
        "  else:\n",
        "    print(\"{} min left\".format(5 - i))\n",
        "    time.sleep(60)\n",
        "    print(\"Czas na odpowieź\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV2L_CN8X_Qd",
        "colab_type": "text"
      },
      "source": [
        "# Sprawdźmy czy macie rację\n",
        "Poniżej zaimportujemy pewien zestaw danych.\n",
        "Korzystając z dokumentacji [`sklearn`](https://scikit-learn.org/stable/) i [`scipy`](https://www.scipy.org/docs.html) będziecie dziś na nim operować, tak aby stworzyć najlepszy możliwy model prostej regresji logistycznej.\n",
        "\n",
        "Dane są wielowymiarowe i należy wyciągnąć z nich jedyne przydatne informacje do klasyfikacji.\n",
        "\n",
        "Dla utrudnienia użyjemy datasetu Wisconsin Breast Cancer bez nazw kolumn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHq8AJovYVHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data & wget http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h2Waj90az1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importujemy bibliotekę pandas by móc działać na data frames\n",
        "import pandas\n",
        "\n",
        "# Wczytujemy plik csv z usunięciem informacji o nazwach kolumn\n",
        "data = pandas.read_csv(\"wdbc.data\", header = None)\n",
        "cols = data.columns.to_list()\n",
        "\n",
        "# Nadajemy \"dummy\" nazwy kolumn\n",
        "for i in range(len(cols)):\n",
        "  if i == 0:\n",
        "    cols[i] = \"Index\"\n",
        "  elif i == 1:\n",
        "    cols[i] = \"Tumor\"\n",
        "  else:\n",
        "    cols[i] = \"Feature {}\".format(i-1)\n",
        "data.columns = cols\n",
        "\n",
        "# Czyścimy co nieco kolumnę kategorii\n",
        "data.Tumor = data.Tumor.replace(\"M\", \"Malignant\")\n",
        "data.Tumor = data.Tumor.replace(\"B\", \"Benign\")\n",
        "column_idx_names = data.columns\n",
        "\n",
        "# Usuwamy ID pacjenta i przywracamy nazwy kolumn zgodne z indeksami\n",
        "data = data.iloc[:,1:]\n",
        "\n",
        "# Wyświetlamy pierwsze 10 rzędów data frame'u\n",
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfzKncmokSvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Printujemy wartości unikalne kolumny zawierającej kategorie\n",
        "print(data.Tumor.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed2kGhgEbGuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Metadane, nie obowiązkowe do zapoznania się\n",
        "with open(\"wdbc.names\",\"rb\") as fh:\n",
        "  for line in fh.readlines():\n",
        "    print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf3n63O2gpJY",
        "colab_type": "text"
      },
      "source": [
        "## Importowanie bibliotek\n",
        "Korzystając z dobrodziejstw Google Collab zaimportujemy biblioteki bez których bylibyśmy jak pijane dziecko we mgle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlILuJAmfrsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "from scipy import stats\n",
        "import itertools\n",
        "from sklearn import preprocessing, linear_model, model_selection, metrics\n",
        "from sklearn import decomposition, ensemble, feature_selection\n",
        "from sklearn.manifold import MDS\n",
        "import numpy\n",
        "from matplotlib import pyplot\n",
        "import operator\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-prcje0vhtXf",
        "colab_type": "text"
      },
      "source": [
        "### **Zadanie 1.**\n",
        "Sprawdź rozkład kolumn z datasetu WBC przy użyciu odpowiedniej funkcji z biblioteki `scipy.stats` (punkt za umiejętność korzystania z dokumentacji)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaVJltBTzPEi",
        "colab_type": "text"
      },
      "source": [
        "## Tworzenie funkcji do sprawdzenia rozkładu zmiennych dla każdej z obecnych klas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyPAnozxhsLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def StratifiedDistributionCheck(dataframe, list_of_tumors):\n",
        "  for tumor in list_of_tumors:\n",
        "    df = dataframe[dataframe.Tumor == tumor]\n",
        "    print(\"Results for {} tumor type\\n -----------------------\\n\".format(tumor))\n",
        "    for column in df.columns:\n",
        "      if column == \"Tumor\":\n",
        "        None\n",
        "      else:\n",
        "        p = stats.shapiro(df.loc[:,column])[1]\n",
        "        print(\"{0:s} Shapiro-Wilk test p-value equals {1:.3f}\\n\".format(\n",
        "            column,p))\n",
        "        if p > 0.05:\n",
        "          print(\"Column is at least sampled from Gaussian distribution\\n\\n\")\n",
        "        else:\n",
        "          print(\"Column's distribuition is not Gaussian\\n\\n\")\n",
        "\n",
        "StratifiedDistributionCheck(data,data.Tumor.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd6W_ePcr3Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uzupełnij kod funkcji przedstawionej poniżej\n",
        "\n",
        "def StratifiedDistributionCheck(dataframe, list_of_tumors):\n",
        "  for tumor in list_of_tumors:\n",
        "    df = dataframe[dataframe.Tumor == tumor]\n",
        "    print(\"Wyniki dla {} tumor type\\n -----------------------\\n\".format(tumor))\n",
        "    for column in df.columns:\n",
        "      if column == \"<NAZWA_KOLUMNY_DO_POMINIĘCIA>\":\n",
        "        None\n",
        "      else:\n",
        "        p = stats.<NAZWA_FUNKCJI>(df.loc[:,column])[1]\n",
        "        print(\"{0:s} <NAZWA_TESTU> test p-value wynosi {1:.3f}\\n\".format(\n",
        "            column,p))\n",
        "        if p > 0.05:\n",
        "          print(\"Twoje wytłumaczenie wyniku działania konstrukcji warunkowej\")\n",
        "        else:\n",
        "          print(\"Twoje wytłumaczenie wyniku działania konstrukcji warunkowej\")\n",
        "\n",
        "StratifiedDistributionCheck(data,data.Tumor.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVevnjI71mwQ",
        "colab_type": "text"
      },
      "source": [
        "### **Zadanie 2.**\n",
        "Wytłumacz co oznaczają powyższe wyniki (punkt za intuicję)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiBZzaKqz4eO",
        "colab_type": "text"
      },
      "source": [
        "# Data Science w praktyce\n",
        "W tej części zobaczycie (i mam nadzieję też zrozumiecie, co z czym się je)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAMXoMs2yCNC",
        "colab_type": "text"
      },
      "source": [
        "## Definiowanie modelu regresji logistycznej z użyciem `sklearn`\n",
        "Stworzymy teraz funkcje zwracające model oraz wynik działania modelu na danych wejściowych, tak aby móc porównać wpływ różnych metod redukcji wymiarów na dane (oraz zobaczyć jak przebiega klasyfikacja w przypadku oryginalnych danych).\n",
        "\n",
        "Najpierw jednak rozbijemy DataFrame na 2 macierze, zawierające dane numeryczne, potrzebne do wytrenowania i testowania modelu, oraz zawierające binarnie enkodowane typy guza."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvZ_rGMt2maT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.iloc[:,1:].values\n",
        "Y_str = data.iloc[:,0]\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "lb.fit(Y_str)\n",
        "print(\"Typy guzów do binaryzacji\\n {} = 0\\n {} = 1\".format(lb.classes_[0],\n",
        "                                                               lb.classes_[1]))\n",
        "Y = lb.transform(Y_str)\n",
        "print(\"\\nRozmiar matrycy X\\n{}\".format(X.shape))\n",
        "print(\"\\nPierwsze dwa rzędy X\\n{}\".format(X[:2,:]))\n",
        "print(\"\\nRozmiar matrycy Y\\n{}\".format(Y.shape))\n",
        "print(\"\\nPierwsze dwa typy guzów Y\\n{}\".format(Y[:2,:]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPpO-UqNrx5M",
        "colab_type": "text"
      },
      "source": [
        "## Sztuczne zwiększenie wymiarowości danych - nie róbcie tego w domu\n",
        "Jako że jest to jeden z prostszych datasetów do nauki ML to żeby nie było za łatwo, dodamy randomowy szum gaussowski i nieparametryczny do danych, tak aby sztucznie zwiększyć ich wymiarowość\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1TY1zZDsLCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AddArtificialData(X, random_state = 2137, art_columns = X.shape[1]):\n",
        "  numpy.random.seed(seed = random_state)\n",
        "  X_artif = numpy.zeros(shape=(X.shape[0], X.shape[1]+art_columns))\n",
        "  for i in range(X_artif.shape[1]):\n",
        "    condition1 = numpy.random.randint(0,2)\n",
        "    if condition1 == 0:\n",
        "      X_artif[:,i] = numpy.random.normal(numpy.random.randint(20,90),numpy.random.randint(5,20),)\n",
        "    else:\n",
        "      condition2 = numpy.random.randint(0,2)\n",
        "      if condition2 == 0:\n",
        "        X_artif[:,i] = numpy.random.rayleigh(scale = numpy.random.randint(1,90),\n",
        "                                             size = X.shape[0])\n",
        "      else:\n",
        "        X_artif[:,i] = numpy.random.rand(X_artif.shape[0])\n",
        "  \n",
        "  X_artif[:,:X.shape[1]] = X\n",
        "  return X_artif\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b-bb0okt8QI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = AddArtificialData(X, random_state = 90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZtTprPb5SDe",
        "colab_type": "text"
      },
      "source": [
        "Poniższa funkcja zwracać będzie gotowy wytrenowany model, wynik działania tego modelu oraz metryki do późniejszego benchmarku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go3VsXxbzKqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LogitModel(X, Y, penalty = \"l1\", reg = 0.2, solver = \"liblinear\",\n",
        "               random_state = 2137, n_iter = 100, split = 0.7, verbose = 1):\n",
        "  Y = Y.ravel()\n",
        "  c = 1 - reg\n",
        "  s = 1 - split\n",
        "  clf = linear_model.LogisticRegression(penalty = penalty,\n",
        "                                        C = 1-reg,\n",
        "                                        solver = solver,\n",
        "                                        random_state = random_state,\n",
        "                                        max_iter = n_iter,\n",
        "                                        verbose = verbose,\n",
        "                                        )\n",
        "  X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y,\n",
        "                                                                      test_size = s,\n",
        "                                                                      random_state = random_state)\n",
        "  \n",
        "  clf = clf.fit(X_train, Y_train)\n",
        "  Y_pred = clf.predict(X_test)\n",
        "  scores = {}\n",
        "  scores[\"Mean Accuracy\"] = clf.score(X_test, Y_test)\n",
        "  scores[\"F1\"] = metrics.f1_score(Y_test, Y_pred)\n",
        "  scores[\"AUC\"] = metrics.roc_auc_score(Y_test, Y_pred)\n",
        "  scores[\"Confusion Matrix\"] = metrics.confusion_matrix(Y_test, Y_pred)\n",
        "  scores[\"ROC Curve\"] = metrics.roc_curve(Y_test, Y_pred)\n",
        "  \n",
        "  pyplot.matshow(scores[\"Confusion Matrix\"])\n",
        "  pyplot.title(\"Confusion Matrix\")\n",
        "  pyplot.colorbar()\n",
        "  pyplot.ylabel(\"True label\")\n",
        "  pyplot.xlabel(\"Predicted Label\")\n",
        "  pyplot.show()\n",
        "\n",
        "  fpr, tpr, _ = metrics.roc_curve(Y_test.ravel(), Y_pred.ravel())\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "  pyplot.figure()\n",
        "  lw = 2\n",
        "  pyplot.plot(fpr, tpr, color='darkorange',\n",
        "          lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
        "  pyplot.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  pyplot.xlim([0.0, 1.0])\n",
        "  pyplot.ylim([0.0, 1.05])\n",
        "  pyplot.xlabel('False Positive Rate')\n",
        "  pyplot.ylabel('True Positive Rate')\n",
        "  pyplot.title('Receiver operating characteristic example')\n",
        "  pyplot.legend(loc=\"lower right\")\n",
        "  pyplot.show()\n",
        "\n",
        "\n",
        "  for i, v in scores.items():\n",
        "    if i == \"Confusion Matrix\":\n",
        "      print(\"{}\\n{}\".format(i,v))\n",
        "    elif i == \"ROC Curve\":\n",
        "      None\n",
        "    else:\n",
        "      print(\"{0:s} metic has value of {1:.3f}\".format(i,v))\n",
        "  return (clf, Y_test, Y_pred, scores)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaoYd7uKPVt5",
        "colab_type": "text"
      },
      "source": [
        "## Redukcja wymiarowości danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UksecyLoIorg",
        "colab_type": "text"
      },
      "source": [
        "### **Zadanie 3**\n",
        "\n",
        "Zastosuj [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), [MDS](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html), [Feature Selection bazujące na drzewach decyzyjnych](https://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection) oraz Infinite Feature Selection na danych wejściowych. Empirycznie dobierz najlepsze hiperparametry w miejscach wskazanych w kodzie. (punkt za eksperymentowanie z kodem)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TAtBhMFSQ4e",
        "colab_type": "text"
      },
      "source": [
        "<u>Dla przypomnienia - wymiarowość pełnego zestawu danych</u>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHSiOzHMSQDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFZatqj2KQ9D",
        "colab_type": "text"
      },
      "source": [
        "#### Infinite Feature Selection - czyli coś nietypowego\n",
        "\n",
        "![obraz.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0q0uI9BTRNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def InfiniteFeatureSelection(X, n_features, alpha):\n",
        "  all_permutations = list(itertools.permutations(range(X.shape[1]), 2))\n",
        "  importance_scores = []\n",
        "  max_var_idx = []\n",
        "  feature_idx = list(range(n_features))\n",
        "  X_important = numpy.zeros(shape = (X.shape[0], n_features))\n",
        "  \n",
        "  for element in all_permutations:\n",
        "    element1 = element[0]\n",
        "    element2 = element[1]\n",
        "    v1 = numpy.var(X[:,element1])\n",
        "    v2 = numpy.var(X[:,element2])\n",
        "\n",
        "    max_var = max(v1,v2)\n",
        "    if max_var == v1:\n",
        "      max_var_idx.append(str(element1))\n",
        "    else:\n",
        "      max_var_idx.append(str(element2))\n",
        "    \n",
        "    spearman_coef, p = stats.spearmanr(X[:,element1], X[:,element2])\n",
        "    reg_max_var = numpy.multiply(alpha,max_var)\n",
        "    reg_spearman_based_coef = numpy.multiply((1 - alpha),\n",
        "                                          (1 - numpy.abs(spearman_coef)))\n",
        "    importance_score =  reg_max_var + reg_spearman_based_coef\n",
        "    importance_scores.append(importance_score)\n",
        "  \n",
        "  data = {\"Max_var_idx\":max_var_idx, \"Importance_scores\":importance_scores}\n",
        "  importance_df = pandas.DataFrame(data = data)\n",
        "  sum_scores = {}\n",
        "  for unq in importance_df.Max_var_idx.unique():\n",
        "    temp_df = importance_df.loc[importance_df.Max_var_idx == unq,:]\n",
        "    colsum = temp_df.Importance_scores.sum(axis=0)\n",
        "    sum_scores[str(unq)] = colsum\n",
        "  sorted_scores = sorted(sum_scores.items(), key=operator.itemgetter(1),\n",
        "                         reverse = True)\n",
        "  most_important_features = list(operator.itemgetter(*feature_idx)(sorted_scores))\n",
        "  for i in range(n_features):\n",
        "    X_important[:,i] = X[:,int(most_important_features[i][0])]\n",
        "\n",
        "  return (X_important, importance_df)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPtGypdlImdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PCA - Manipuluj wartością \"n_components\"\n",
        "\n",
        "start = time.time()\n",
        "pca = decomposition.PCA(n_components = 10, random_state = 2137, svd_solver=\"auto\")\n",
        "pca.fit(X,Y.ravel())\n",
        "for i in range(len(pca.explained_variance_ratio_)):\n",
        "  print(\"\\nExplained Variance for PC{0:}: {1:.2f}%\".format(i,pca.explained_variance_ratio_[i]*100))\n",
        "X_pca = pca.transform(X)\n",
        "print(\"\\nPCA zajęło {} sekund\".format(time.time() - start))\n",
        "\n",
        "#MDS - Manipuluj wartościami \"n_components\", \"dissimilarity\" i \"max_iter\"\n",
        "\n",
        "start = time.time()\n",
        "mds = MDS(n_components = 10, random_state = 2137,\n",
        "          dissimilarity = \"euclidean\", max_iter = 300)\n",
        "X_mds = mds.fit_transform(X)\n",
        "print(\"\\nMDS zajęło {} sekund\".format(time.time() - start))\n",
        "\n",
        "#Decision Tree - Manipuluj wartością \"n_estimators\"\n",
        "\n",
        "start = time.time()\n",
        "tree_clf = ensemble.ExtraTreesClassifier(n_estimators = 1000,    \n",
        "                                         random_state=2137)\n",
        "tree_clf = tree_clf.fit(X, Y.ravel())\n",
        "feature_model = feature_selection.SelectFromModel(tree_clf, prefit = True)\n",
        "X_tree = feature_model.transform(X)\n",
        "print(\"\\nUtworzenie drzew zajęło {} sekund\".format(time.time() - start))\n",
        "\n",
        "#Infinite Feature Selection - Manipuluj wartościami n_features oraz alpha\n",
        "\n",
        "start = time.time()\n",
        "X_inf, Inf_df = InfiniteFeatureSelection(X, n_features = 10, alpha = 0.3)\n",
        "print(\"\\nInfinite Feature Selection zajęło {} sekund\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSn3wH51IVq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Zadanie 4** \n",
        "\n",
        "Zastosuj funkcję `LogitModel` na danych otrzymanych w zadaniu trzecim i określ który model dał najlepsze wyniki oraz jaka może być tego przyczyna. Możesz testować pomysły z zadania 3. (punkt za rozumowanie badawcze i umiejętność wyciągania wniosków)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbzHdPRRPcFC",
        "colab_type": "text"
      },
      "source": [
        "<b>Wyniki dla X (pełnowymiarowych danych)</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSdiNLPjPifF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_X = LogitModel(X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fblgm9ESOl9S",
        "colab_type": "text"
      },
      "source": [
        "<b> Wyniki dla X_pca </b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BxxC_fxOkt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_PCA = LogitModel(X_pca, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iWBExMkOrsj",
        "colab_type": "text"
      },
      "source": [
        "<b>Wyniki dla X_mds</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSzTqjj-Ov4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_MDS = LogitModel(X_mds,Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7LoqN7sOwrh",
        "colab_type": "text"
      },
      "source": [
        "<b>Wyniki dla X_tree</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6GgGBlpO0du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_TREE = LogitModel(X_tree,Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17D-4v-jjid",
        "colab_type": "text"
      },
      "source": [
        "<b> Wyniki dla X_inf</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p35kOtWBS3Pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_Inf = LogitModel(X_inf,Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGAIILrU0zxB",
        "colab_type": "text"
      },
      "source": [
        "### Zadanie 5 - Machine Learning na bogato // Playground\n",
        "\n",
        "Rozwiąż powyższe zadanie używając biblioteki tensorflow z wysokopoziomowym API \"Keras\" - wypełnij luki w kodzie oraz dobierz najlepsze hiperparametry. Punktów nie ma ale też jest zaje..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypwNeb4c04tV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-81aXgP2fRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = X_tree.shape[1]\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(100, activation = 'relu', input_dim = input_dim))\n",
        "model.add(Dense(200, activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(100, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"Adam\", metrics = [\"AUC\"])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-N5ZlrG4PqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_tree, Y, epochs = 500, validation_split = 0.3, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwt9vvw05STT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vex5LrLW6EKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}